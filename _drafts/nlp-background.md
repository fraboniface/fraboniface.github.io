---
layout: post
title: Catching up on Natural Language Processing with Deep Learning
author: Fran√ßois
---


Are you beginning in NLP, or just didn't follow the latest advances? Wondering about OpenAI's GPT2, the neural net that was too dangerous to be released? This article is for you.
If you don't know what NLP is or what machine learning is, you might have troubles understanding it.
This is a technical article for machine learning students and practitioners. I initially wrote it for my colleagues.

Let's start!

# Introduction
Natural Language Processing is the field of machine learning that deals with language as humans speak or write it. It has of course roots in linguistics but it is now dominated by statistical models, i.e. machine learning. With vision, it is one the fields at which humans are so good that we don't realize how hard it is. Consequently, like computer vision, it is now dominated by neural networks. Compared to classical machine learning models, they excel at tasks that we find intuitive, i.e. that are processed by our brain unconsciously. I will therefore focus specifically on NLP with deep learning.

[A detailed tutorial on the matter](https://arxiv.org/abs/1510.00726) already exists, but it's quite long and a few years old now. A shorter and more recent one cannot hurt. You can also find a lot of information on [Sebastian Ruder's blog](http://ruder.io/). I know about deep learning and have read enough about deep learning models for NLP to tell you about it, but I don't have the knowledge to tell you about the linguistics side of things, which is necessary nowadays only for specific tasks such as part-of-speech tagging and constituency parsing.


This is a good time to get interested in NLP because 2018 has seen major breakthroughs. People finally obtained good performance in transfer learning, something that has been possible for years in computer vision, with models trained on ImageNet. Recent powerful *Universal Language Models* can be trained unsupervised on large amounts of text data and then be reused with minimal fine-tuning for downstream tasks.


# NLP tasks, datasets and evaluation
NLP has a large variety of tasks. Each of them represents a step towards machines understanding human language. Some of the main ones, along with the corresponding datasets and metrics, are listed below:
- language modelling: training a language model amounts to building a generative model for language (see [Language Model](#language-model) for details). It is one of the most generic and popular NLP tasks. It allows natural language *generation*. It is an unsupervised task, meaning one can train a language model on corpora of text found on the internet. It makes language modelling one of the few tasks in AI for which virtually unlimited data is available. To train GPT2 (see [this part](#universal-language-models-and-transfer-learning)), OpenAI used 40GB of text scraped on the internet[^1].
The metric reported is the perplexity, which is the exponential of the entropy of the distribution defined by the model on the text. Intuitively, it represents how confused (perplexed) the model is in predicting the next word: a perplexity of $$k$$ means the model is as uncertain as if it had to choose uniformly at random between $$k$$ words. Lower is better;
- machine translation (MT): the acronyms SMT and NMT are often used to talk about Statistical MT and Neural MT. NMT consistently outperforms SMT nowadays, at the cost of poorer interpretability. Results are heavily influenced by how much data there is in a given language pair, but [training MT models without parallel data](https://arxiv.org/abs/1710.04087) is an active research area. International organizations such as [the EU](http://www.statmt.org/europarl/) and [the UN](https://cms.unov.org/UNCorpus/) are excellent sources of high-quality translations. The metric used for assessing translation quality is [BLEU](https://www.aclweb.org/anthology/P02-1040) (BiLingual Evaluation Understudy). It compares the model's output to a number of reference human translation and quantifies how well $$n$$-grams overlap. Higher is better. The [METEOR metric](https://www.cs.cmu.edu/~alavie/papers/BanerjeeLavie2005-final.pdf) is sometimes preferred;
- sentiment analysis: classifying to what sentiment a piece of text corresponds. The popular [IMDb dataset](https://ai.stanford.edu/~ang/papers/acl11-WordVectorsSentimentAnalysis.pdf) asks to classify whether movie reviews are positive or negative;
- question answering (QA): there are several tasks of QA. Most relevant to deep learning is *extractive* QA, meaning the answer to the question is a chunk of the contextual text given to the model. Other types include *abstractive* QA, where the model must generate the text of the answer, knowledge-base and logic-base QA. The most popular datasets are SQuAD and the more recent SQuAD 2.0. A leaderboard of the best-performing models is maintained [here](https://rajpurkar.github.io/SQuAD-explorer/). The metrics reported are the percentage of exact match between the model's answer and one of the target answers, and the F1 score;
- named entity recognition (NER): the task of tagging entities in text with types. For instance, a NER system can be used to find people or places mentioned in a piece of text. F1 score is used.
- summarization: contracting a document into a shorter version while preserving meaning. To automatically assess summary quality, the [ROUGE](https://www.aclweb.org/anthology/W04-1013) (Recall-Oriented Understudy for Gisting Evaluation) score is used. Several versions of ROUGE (ROUGE-$n$, ROUGE-L, etc.) are available, each mesuring recall between subsequences of model prediction and reference summaries.

This list if far from exhaustive. A good resource to find datasets as well as SOTA results for a large number of NLP tasks is <http://nlpprogress.com>.

[^1]: For an open-source reproduction of this corpus: <https://skylion007.github.io/OpenWebTextCorpus/>.

# Text preprocessing
## Classical preprocessing steps
In order to ease the task of computers, it is usual to clean the text as preprocessing. The first step is tokenization, separating the text into tokens. A token is the basic unit of sense for the model, usually words but sometimes characters or subwords. Then, stop words are sometimes removed, depending on the application. Stop words are very frequent words with a grammatical function but that add little meaning to the sentence (*e.g* "the" and "of"). For basic sentiment analysis for instance, one doesn't need to know the precise grammatical structure of the sentence as long as informative words like "excellent" or "terrible" are kept. Of course this exposes us to missing more subtle formulations, especially if a word like "not" is removed. Depending on the application, you can also stem or lemmatize the word, i.e. replace the word by its root. This amounts to removing the conjugation of verbs, the declination of words and in some cases replacing the word with a more common synonym ("automobile" becomes "car"). One can also correct spelling, if bad spelling can hurt model understanding.

Text preprocessing seems to be becoming somewhat outdated in NLP with deep learning. Tokenization remains necessary---even humans cut text into basic pieces, but now that truly enormous amounts of text are being used to train ever larger models, the computation overhead of preprocessing (especially lemmatization) becomes both problematic and unnecessary. Indeed, the impressive statistical power of recent deep learning models, fed with more text than a human could read in a lifetime, allows these models to learn the meaning of all different conjugations, declensions or misspellings that are sufficiently common to be relevant.


## Word embeddings
Computers deal with numbers, not with human words. We need to encode the latter into computer language. The naive way to do so is one-hot encoding, also known as the *{localist* representation. Each word has a specific index and is represented as a very sparse vector of the size of the vocabulary, whose only nonzero element is a one at the word's index. It is extremely high dimensional, and all word vectors are orthogonal to each other. Thus, it is impossible to define a notion of similarity. The words are treated as unrelated categories. This is obviously unsatisfying, so more sensible vector representations have been designed. [Word2vec](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) is a *distributed* vector representation, built on the distributional hypothesis[^2]. There are many resources to explain Word2vec, but I'm going to suggest [Jay Alamaar's blog post](http://jalammar.github.io/illustrated-word2vec/) because other posts of his blog will be useful later. The basic idea is to train a linear neural network to predict a word given its context (CBOW) or the context given a word (*skip-gram*). The context here contains the few words surrounding the word in question. It is actually equivalent to extracting the principal components of the co-occurrence matrix with a few additional tricks, which is called [GLoVe](https://nlp.stanford.edu/projects/glove/). Using distributed word embeddings allowed to significantly improve the performances of most NLP models. It was an early kind of transfer learning, although a shallow one, as a single layer is reused (the embedding layer).

[^2]: "You shall know a word by the company it keeps," J.R. Firth

# Language Models and RNNs
## Language Model
A language model (LM) is a generative model for language, i.e. it can estimate the probability of a word appearing by itself, of a word being the next word given the previous words, or even that of a sentence. It defines a probability distribution over the vocabulary $$V$$. Smart keyboards use simple ones to give next words suggestions. Google uses a more sophisticated one for query suggestion and in its new Gmail feature. If you're trying to predict the word at position $$t$$ given the previous words, the model gives you $$P(w_t | w_0, w_1, ..., w_{t-1})$$. If you want the probability of a sentence, the chain rule of probabilities allows you to write it as : $$P(w_{0:t}) = \prod_{l=0}^{t}P(w_{l+1}|w_{0:l})$$. Before Recurrent Neural Networks (RNNs) [really took off](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), the use was to assume that a word at position $$t$$ only depended on the $$n$$ words preceding it. This is called the Markov assumption. It allows one to write: $$P(w_t | w_{0:t-1}) = P(w_t | w_{t-n:t-1})$$. For reasonable $$n$$, e.g. 2 or 3 (5 is the maximum), one can simply count all the occurrences of the $$n$$-grams (tuples of $$n$$ words), and estimate probabilities by normalizing by the total word count. If one wanted a fancier model, one could use a Hidden Markov Model (HMM), which exploited the Markov property with hidden variables. Although the Markov assumption is not entirely unreasonable --- that's what smart keyboard use and their pretty useful --- we can easily see its limits when we want to deal with long term dependencies.

**hmm figure goes here**

## Recurrent neural networks and LSTMs
RNNs maintain an internal state that is updated at each new token. It is updated and not replaced, meaning it theoretically lifts the Markov assumption. However, due to gradient vanishing issues, vanilla RNNs are not great in practice. We use instead [Long Short-Term Memory](https://www.bioinf.jku.at/publications/older/2604.pdf) (LSTM) cells. Those are fancier RNNs designed to solve the gradient vanishing problem and have better long-term memory. They are a bit complicated, but you can read [the great blog post by Chris Olah](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) about them. More recent versions such as the [Gated Recurrent Unit (GRU)](https://arxiv.org/abs/1406.1078) have been proposed but LSTMs remained popular until transformers arose (see [this section](#attention-and-the-transformer)).

The training of an RNN for language modelling is as follows: at each time-step, the model is aware of the previous words and makes a prediction for the next one. This prediction comes from a softmax outputting a discrete distribution over the entire vocabulary. Given the true next word, one can compute the cross-entropy loss, just as in normal classification. The losses for the different time-steps are then back-propagated through time (BPTT). Disappointingly, this term simply refers to the unrolling of the cell along the different time-steps (represented below). During BPTT, the weights matrix from the internal state to itself is multiplied by itself many times, as well as the gradient, which makes the training of vanilla RNNs unstable. LSTMs also solved this problem.

**rnn image goes here**

## Sub-word models
Although using entire words as tokens is most straightforward, especially in a language like English, this approach has shortcomings. How does one deal with out-of-vocabulary words or informal spelling? Humans can often get what a word means by looking at its spelling, even if it has not been seen before. Using words as the smallest unit of sense in a LM doesn't allow that. In Chinese, words are not space-separated. In German, words are composed to form nouns, the meaning of which is easily inferred from their constituent words. In languages with cases, a noun is declined according to grammatical rules but keeps its meaning. To address these challenges, there has been some research on using smaller units than words as tokens. There are three main strategies: working at the character level, using character $$n$$-grams (tuples of $$n$$ characters), or using a hybrid model that models both words and sub-word units.

[Character-level LMs](https://arxiv.org/abs/1308.0850) work, and are the default way to go for ideogram-based languages such as Chinese. They also obtain [very good results](https://arxiv.org/abs/1511.04586) on morphologically-rich languages, such as German or Slavic languages. They are very slow, however, because the model processes about seven times more tokens and BPTT needs to be performed much further back in time than with words. Models based on character $$n$$-gram are a good compromise since morphemes[^3] are usually a few characters long. Nevertheless, pure sub-word models can be too biased towards syntax and spelling. For instance, they would translate proper names, which is sometimes desired but not always. [Hybrid models](https://arxiv.org/abs/1604.00788) have been developed to combine the power of sub-word models for unknown or rare words and that of word-level models for common words. 

[^3]: In linguistic, a morpheme is the smallest unit of sense.} 


# Advanced architectures
## Seq2seq, Bi-LSTMS, stacked LSTMs
The straightforward way to use an LSTM is to pass it over a sentence and use the cell's last internal state for further computation (*e.g.* a simple linear classifier). This architecture is known as an encoder, because it encodes the sentence into a fixed-size vector. One can also form a decoder: from an initial vector state, the LSTM emits an output, which is re-fed as an input to the cell for the next step. The internal state of the lSTM is updated in the process. The decoder keeps emitting new tokens until it emits a specific stop signal. If the initial state is random, then one has a simple text generator, but otherwise the decoder forms a conditional language model able to generate text related to the information encoded in the initial state. We can combine an encoder and a decoder to get what is creatively called a *sequence-to-sequence* (seq2seq) or *encoder-decoder* architecture (same paper as GRU). The encoder maps the input from one domain to a common representation, and the decoder maps it from the common representation to another domain. This applies very naturally to machine translation (MT, illustrated below), but also to [image captioning](https://arxiv.org/abs/1411.4555) if the encoder is a computer vision model. An encoder-decoder model can be learned end-to-end, since the gradient can flow from the output of the decoder to the first layer of the encoder just as in a simpler neural network. This allows both parts to learn automatically the common representation.


**seq2seq figure here**

However, LSTMs are not perfect, and in particular, they don't have such a good long-term memory. They can forget the beginning of the sentence by the time they get to the end. For MT, the simple trick of reversing the reading direction of the encoder improves performance; the start of the original sentence is then very close to that of its translation. This doesn't solve the so-called *bottleneck* problem, however, which is that encoding sentences of varied lengths into fixed-size representations is quite hard. A solution is to use a bidirectional LSTM (Bi-LSTM). A Bi-LSTM is actually two LSTMs, one reading from left to right and the other from right to left. Their internal states are concatenated. A schematic representation of one is available below. The problem is that a Bi-LSTM doesn't produce a fixed-size representation anymore, which calls for additional design choices (*e.g.* a pooling function or an attention mechanism, see [Attention and the Transformer](#attention-and-the-transformer)). Finally, we should note that an LSTM, or even a Bi-LSTM, is still a shallow neural network when considered at each time-step: it comprises only one layer, or two counting the word embedding layer. This is in stark contrast to the CNNs with dozens of layers used in modern CV. To enable the abstract representations that depth permits, one can stack LSTMs on top of each other.

**biLSTM here**

## Convolutional neural networks for NLP
Another option to address the bottleneck problem is to look at a sliding window of a certain size, compute an encoding at each position, and pool over the positions to get a fixed-size encoding. This is similar to pooling the hidden states of an LSTM, except that one can look at several words at once, *i.e.* $$n$$-grams. The major advantage is that it is parallelizable, unlike an RNN. Computing something over a sliding window is in essence a 1D-convolution, so the network is now a CNN. Because CNNs are much faster than RNNs, we can also make them deeper---one rarely stacks more than three LSTMs. Good results have been obtained in this way, but it seems CNNs never established themselves as the way to go for NLP, probably because of transformers.

CNNs are also useful for sub-word models. They speed up computations and the concept of sliding window naturally lends itself to character $$n$$-grams.

## Attention and the Transformer
Among the design choices available to map the internal states of a Bi-LSTM to a fixed-size representation, one can compute the mean or max element-wise, concatenate both, or use *attention*. Attention allows the model to attend more or less to words depending on a *query*. When you have a text in front of you and someone asks you a question about its content, you will pay attention to different words in the text, and just before answering, your attention will be maximally focused on the few words that you think give the answer. Attention in deep learning stems from that idea. In a neural network, an attention mechanism forms a weighted average of the internal states at different time-steps. The weights are the attention coefficients, and are computed using a softmax on attention scores. The latter are obtained, for instance, as the dot product of the corresponding internal state and a *query* vector. For question answering, this query vector may be an encoding of the question. For MT, it would be the internal state of the decoder at the current time-step. Attention scores can also be computed in other ways, *e.g.* with a learnable weight matrix in the middle of the two vectors.

**attention here**

As I said, the inherently sequential nature of LSTMs and Bi-LSTMs make them hard to parallelize, and storing all the states of Bi-LSTSMs raises memory issues. To solve this, Google researchers designed [the Transformer](https://arxiv.org/abs/1706.03762). An excellent resource to understand this paper is [Alexander Rush's article](http://nlp.seas.harvard.edu/2018/04/03/attention.html). In addition, Jay Alamaar has again [a nice blog post](http://jalammar.github.io/illustrated-transformer/) about it. The Transformer is essentially an encoder-decoder architecture, but it is purely based on attention mechanisms instead of recurrence and convolutions. It comprises six stacked encoder layers, each more complicated than a vanilla LSTM encoder. The output of the sixth encoder layer is fed to a decoder of the same depth. The encoder blocks are bidirectional, whereas the decoder blocks receive masked inputs in order to ensure that an output at position $$i$$ only depends on known outputs at position less than $i$. The architecture of a Transformer is shown below. It is highly parallelizable, and yields much better performance than RNN-based architecture on a number of NLP tasks, especially when dealing with long-term dependencies. One of the main advances is *multiheaded attention*: instead of having one set of attention coefficients (one attention head), a transformer uses several. This allows it to deal with several queries at the same time: what is the subject of the sentence, what is the object, what verb are they linked to, and so forth. Transformers are now SOTA on most NLP tasks. They have also been used for [image](https://arxiv.org/abs/1802.05751) and [music](https://arxiv.org/abs/1809.04281) generation. Extensions of the Transformer have also been proposed, for instance in [this work](https://arxiv.org/abs/1807.03819) or [this one](https://arxiv.org/abs/1904.10509). The latter resulted in OpenAI's impressive [MuseNet](https://openai.com/blog/musenet/).

**transformer**

# Universal language models and transfer learning
## ELMo and ULMFiT
Transfer learning (TL) is the concept of using the knowledge gained from training on a *source* task for solving a *target* task, for which less data is available, or simply when we don't want to train a model from scratch. Until recently, TL beyond word embeddings did not work well for NLP. Pretrained models were subject to *catastrophic forgetting*, meaning the model forgot all of its previous training during fine-tuning. This changed in 2018, with several papers achieving good TL performance. They are all based on so-called *universal language models* (ULMs). ULMs are simply language models trained on very large text corpora with no bias towards a specific task or theme, *e.g.* the whole of Wikipedia for one language (or several), or the [BookCorpus](https://arxiv.org/abs/1506.06726), or both. The task of learning to predict the next word on such a large and varied body of text is very general (universal), as much as, if not more than, ImageNet classification in CV. It is thus an excellent source task for TL.

Given this general task, we still need a way to actually transfer the knowledge gained in the ULM training to the target task. There are two directions to this: contextual word embeddings and the fine-tuning process itself.

The first was introduced by [Embeddings from Language Models (ELMo)](https://arxiv.org/abs/1802.05365). A similar idea has been proposed [in the context of MT](https://arxiv.org/abs/1708.00107). It addresses the problem of polysemy: classical word embeddings cannot distinguish between the different meanings of the same word. The word "stick" will be represented by the same vector no matter what meaning of "stick" is being used. In practice, this vector will be an average of the different meanings of "stick", weighted by their frequency in the corpus used for training the word embeddings. This is a hurdle to language understanding by the model. To improve the latter, the word needs to be placed in context, *i.e.* associated with its neighbours. If you think of it, that's actually what LSTMs and Bi-LSTMs do. Realizing this, the authors of ELMo trained two ULMs, a forward one and a backward one, using 3 stacked Bi-LSTMs. After training, the weights are frozen. When one wants to use ELMo for transfer learning, one then runs the frozen model over the target corpus, and gets three vectors for each word, one per LSTM layer. They are all contextual word embeddings, but capture different levels of abstraction---syntactic for the lower layer, semantic for the higher one. During TL, a weighted average of these vectors will be formed, with the weights learned from the target task. The resulting vector is the embedding of the corresponding word *in context*, and adapted to the target task. One can use ELMo as a drop-in replacement to vanilla word embeddings with any downstream model. This leads to impressive performance gains on virtually all NLP tasks.

The second, [Universal Language Model Fine-Tuning (ULMFiT)](https://arxiv.org/abs/1801.06146), is a set of techniques that enabled reliable fine-tuning of ULMs. Contrary to ELMo, one can use ULMFiT to directly fine-tune the ULM itself to adapt to the target task instead of relying on a downstream model. It is not incompatible with ELMo but rather complementary. Still, it seems a more efficient kind of TL than pure contextual word embeddings, since the same model is used. This is illustrated by the great sample-efficiency of this technique: 100 datapoints are often sufficient to get good performance on the target task.

Using these techniques, one can train a big model once and then obtain quickly excellent performance on a number of tasks, just like ResNet models trained on ImageNet have been used for years in CV.

## A battle of giants: BERT, GPT and GPT2, XLNet, and RoBERTa
The best-performing architectures for NLP nowadays are a mix of everything that works well: scaled-up Transformers trained as ULMs on subwords, that can be fine-tuned using ULMFiT ---or not--- and provide contextual word embeddings, as well as sentence embeddings. Their names are GPT, BERT and GPT2. [OpenAI's GPT](https://openai.com/blog/language-unsupervised/) consists of 12 decoder blocks of a Transformer, without an encoder. [Google's BERT](https://arxiv.org/abs/1810.04805) instead consists of 12 *encoder* blocks. Consequently, it has about the same number of parameters as GPT, but is bidirectional. The careful reader might wonder if a bidirectional LM doesn't cheat by predicting words it has already seen. This is true, which is why the authors don't train BERT on next token prediction. They instead mask 15\% of the tokens and ask the model to guess those, solving the problem. If you're wondering whether ELMo didn't have this problem, it didn't: the forward and backward LM in ELMo are independent, whereas BERT is a single bidirectional LM. As an additional objective, the model has to tell whether a given candidate sentence follows the current one in the text, or is unrelated. This supposedly forces it to somewhat understand how sentences relate to each other, although we will see this has been questioned. BERT Large is a scaled-up version of BERT, with 24 encoder blocks. It is SOTA on a number of tasks. If you think BERT Large is huge, you're right: 340M parameters. But OpenAI replied with [GPT2](https://openai.com/blog/better-language-models/): a direct scale-up of GPT with *48* decoder blocks and over *1.5B* parameters. The model was so good at language generation that OpenAI didn't release it, for fear of misuse. Once again, I link to Jay Alamaar's blog for [a good article](http://jalammar.github.io/illustrated-bert/). Obviously, these models take forever to train, especially if you don't have TPUs. It is not that much of a problem, though, since we don't need to train them from scratch but only to use them for TL. Pre-trained versions are available on the web (except GPT2, but you can find smaller versions of it). 

Since then, [XLNet](https://arxiv.org/abs/1906.08237) and [RoBERTa](https://ai.facebook.com/blog/roberta-an-optimized-method-for-pretraining-self-supervised-nlp-systems/) came up. The first is BERT but with a fancier loss function, and it was SOTA for a time. Then came RoBERTa, which is also BERT, simply better trained. It doesn't use the next sentence prediciton task, questioning its usefulness.

**talk about distillation and small berts, and conclude**